# NEBULA Unified Data Platform

NEBULA provides a unified, modular, and extensible dataset platform for embodied AI and robotics research. It provides a clean and scalable interface for working with large-scale Vision-Language-Action (VLA) datasets â€” including multi-modal observations, robot control trajectories, and language instructions â€” in a structured and robot-agnostic way.

NEBULA Data Platform is able to

- Supports multi-robot datasets

- Fully compatible with single-arm or dual-arm setups

- Designed for both simulation and real-world data

- Plug-and-play for learning, benchmarking, and evaluation

## ðŸ”§ What is this?

This section of the NEBULA codebase implements the data loading and representation layer. It defines how standard NEBULA dataset files (e.g., data.h5 + meta.json) are:

1. Discovered and parsed from disk

2. Translated into structured Python objects (Episode, Step, Observation, etc.)

3. Decoded into robot-specific joint/action/image data using a config system

4. Made accessible through a powerful, queryable Python SDK

## ðŸ§  How it works?

### Structured Episode Format

At the core is a standard representation for robot episodes:

```markdown
Episode
â”œâ”€â”€ LanguageInstruction  (text command)
â”œâ”€â”€ List[Step]
â”‚   â”œâ”€â”€ Observation (images, depth, robot state)
â”‚   â””â”€â”€ Action (joint or symbolic control)
â”œâ”€â”€ MetaInfo (task, environment, success)
â”œâ”€â”€ Statistics (optional: min/max/mean/std)
```

An Episode represents one complete trajectory of an agent (robot) interacting with the environment under a specific task, usually paired with a language instruction, a sequence of multimodal observations, actions, and optional metadata/statistics.

#### instruction: LanguageInstruction

```markdown
LanguageInstruction
â”œâ”€â”€ raw_text: str                       # e.g., "Pick up the red cube and place it on the green platform"
â”œâ”€â”€ tokenized: List[str]                # e.g., ["Pick", "up", "the", "red", "cube", ...]
â”œâ”€â”€ embedding: Optional[np.ndarray]     # e.g., BERT or CLIP embedding vector (optional)
```

#### steps: List[Step]

Each Step corresponds to one timestep in the trajectory.

```markdown
Step (for each timestep t)
â”œâ”€â”€ observation: Observation
â”œâ”€â”€ action: Action
â”œâ”€â”€ timestamp: float                # Optional: real-time or simulation time
```

#### observation: Observation

This contains all the sensor data at time $t$, including images, depth maps, and robot states.

```markdown
Observation
â”œâ”€â”€ views:
â”‚   â”œâ”€â”€ base_camera: RGBImage
â”‚   â”œâ”€â”€ hand_camera: RGBImage
â”‚   â””â”€â”€ ... (other views as defined in config)
â”‚
â”œâ”€â”€ state:
â”‚   â”œâ”€â”€ arm: np.ndarray              # shape (DOF,) e.g., [q0, q1, ..., q6]
â”‚   â”œâ”€â”€ gripper: np.ndarray          # e.g., [open_amount]
â”‚   â””â”€â”€ ... (other limbs if present)
```

Each view (e.g., base_camera) is typically a `RGBImage: np.ndarray of shape (H, W, 3), dtype=uint8`

#### action: Action

This contains the control command issued at time $t$.

```markdown
Action
â”œâ”€â”€ arm: np.ndarray                 # shape (DoF,), e.g., desired joint velocity or position
â”œâ”€â”€ gripper: np.ndarray             # e.g., single value for open/close
â”œâ”€â”€ raw: Optional[np.ndarray]       # Flat concatenated array if needed
```

#### meta: MetaInfo

Contains metadata about the episode (task, environment, success, etc.).

```markdown
MetaInfo
â”œâ”€â”€ task_name: str                 # e.g., "Control-Easy-PlaceSphere"
â”œâ”€â”€ subtask_id: str                # e.g., "0034"
â”œâ”€â”€ robot_name: str                # e.g., "franka_panda_single_arm_2gripper"
â”œâ”€â”€ success: bool                  # Whether the task was completed successfully
â”œâ”€â”€ env_id: Optional[str]          # Environment hash or ID
â”œâ”€â”€ trajectory_id: Optional[str]   # Unique identifier (for cross-referencing)
â”œâ”€â”€ episode_idx: int               # Index in dataset
```

#### stats: Optional[Stats]

Optional summary statistics, auto-computed or cached.

```markdown
Stats
â”œâ”€â”€ step_count: int
â”œâ”€â”€ state_min: np.ndarray
â”œâ”€â”€ state_max: np.ndarray
â”œâ”€â”€ state_mean: np.ndarray
â”œâ”€â”€ action_min: np.ndarray
â”œâ”€â”€ action_max: np.ndarray
â”œâ”€â”€ action_mean: np.ndarray
â”œâ”€â”€ task_difficulty: Optional[str]  # e.g., "easy", "medium", "hard"
```

This makes it easy to train models, evaluate behavior, or inspect data.

### Robot-Abstracted Embodiment Layer

Robot properties are not hardcoded. Instead, theyâ€™re defined in a centralized config (`embodiment_configs.py`) and loaded via the `Embodiment` class (`embodiment.py`).

```python
Embodiment("robot_configs.py", "franka_panda_single_arm_2gripper")
```

This enables NEBULA to handle:

1. Different numbers of joints
2. Different gripper types (1-finger, 2-finger)
3. Multi-arm configurations (e.g., left/right arms)
4. Different state/action slicing rules
5. Different sensor views (cameras)

### NEBULA Dataset Folder Structure

The NEBULADataset class loads raw .h5 files and .json metadata from:

```markdown
dataset_root/
â””â”€â”€ <task_name>/
    â””â”€â”€ motionplanning/
        â””â”€â”€ <subtask_id>/
            â”œâ”€â”€ data.h5
            â””â”€â”€ meta.json
```

### High-Level SDK for Querying & Sampling

The NEBULADatasetSDK exposes a clean Python API:

```python
from nebula.dataset.nebula_sdk import load_nebula_dataset

sdk = load_nebula_dataset("path/to/data")

# Get a successful episode for a given task
ep = sdk.episodes().task("stack_cube").success(True).first()

# Sample 10 episodes
batch = sdk.sample(10)

# Train-test split
train_ds, test_ds = sdk.train_test_split()
```

You can filter by task or subtask, success or failure, instruction text (natural language), step count, or robot type

## ðŸš€ How to use it?

### Load the dataset

```python
from nebula.dataset.nebula_sdk import load_nebula_dataset

dataset = load_nebula_dataset(
    dataset_root="/path/to/nebula",
    robot_config_path="robot_configs.py",
    robot_name="franka_panda_single_arm_2gripper"
)
```

### Query and Access Data

```python
# Get an episode
episode = dataset.get_episode(0)

# Filter for successful long-horizon episodes (This make take long time)
results = dataset.episodes().success(True).min_steps(100).execute()

# Iterate over episodes for a specific task
for ep in dataset.episodes().task("pick_cube"):
    print(ep.instruction.raw_text)
```

### Explore Metadata

```python
# Random sampling
batch = dataset.sample(5)

# Train/test split (stratified by task)
train_ds, test_ds = dataset.train_test_split(test_size=0.2)
```