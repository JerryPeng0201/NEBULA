# ðŸŽ¯ Baseline Model Adapters

Standardized adapters for integrating Vision-Language-Action models with NEBULA.

## ðŸ“‹ Available Baselines

| ðŸ¤– Baseline | Fine-tuning | Inference & Visualization | Checkpoint |
|-------------|-------------|---------------------------|------------|
| [GR00T-1.5](/baselines/gr00t/) | âœ… | âœ… | ðŸš§ |
| [SpatialVLA](/baselines/SpatialVLA/) | âœ… | âœ… | ðŸš§ |
| [RDT-1B](/baselines/RDT/) | âœ… | âœ… | ðŸš§ |
| [Diffusion Policy](/baselines/DP/) | âœ… | âœ… | ðŸš§ |
| MT-ACT | ðŸš§ | ðŸš§ | ðŸš§ |
| ACT | ðŸš§ | ðŸš§ | ðŸš§ |

**TODO:** We are going to release more model adapters and their checkpoints soon.

## ðŸ”§ Add Your Model

Use the **[Custom Template](./custom/README.md)** to integrate your own VLA model:

1. Implement three functions in `model_simulation.py`
2. Configure `config.yaml`
3. Run `bash run_nebula_test.sh`

## ðŸ“Š Compare Models

Generate graphs for multiple model comparsion:

```bash
python nebula/visualization/generate_graph_multiple.py --all
```

---

See individual model folders for detailed documentation.